{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Copyright (c) Microsoft Corporation. All rights reserved.\n",
        "\n",
        "Licensed under the MIT License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Automated Machine Learning\n",
        "_**Text Classification Using Deep Learning**_\n",
        "\n",
        "## Contents\n",
        "1. [Introduction](#Introduction)\n",
        "1. [Setup](#Setup)\n",
        "1. [Data](#Data)\n",
        "1. [Train](#Train)\n",
        "1. [Evaluate](#Evaluate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n",
        "This notebook demonstrates classification with text data using deep learning in AutoML.\n",
        "\n",
        "AutoML highlights here include using deep neural networks (DNNs) to create embedded features from text data. Depending on the compute cluster the user provides, AutoML tried out Bidirectional Encoder Representations from Transformers (BERT) when a GPU compute is used, and Bidirectional Long-Short Term neural network (BiLSTM) when a CPU compute is used, thereby optimizing the choice of DNN for the uesr's setup.\n",
        "\n",
        "Make sure you have executed the [configuration](../../../configuration.ipynb) before running this notebook.\n",
        "\n",
        "An Enterprise workspace is required for this notebook. To learn more about creating an Enterprise workspace or upgrading to an Enterprise workspace from the Azure portal, please visit our [Workspace page](https://docs.microsoft.com/azure/machine-learning/service/concept-workspace#upgrade).\n",
        "\n",
        "Notebook synopsis:\n",
        "1. Creating an Experiment in an existing Workspace\n",
        "2. Configuration and remote run of AutoML for a text dataset (20 Newsgroups dataset from scikit-learn) for classification\n",
        "3. Evaluating the final model on a test set\n",
        "4. Deploying the model on ACI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.PipelineRun = azureml.pipeline.core.run:PipelineRun._from_dto with exception (azureml-core 1.3.0 (c:\\programdata\\anaconda3\\lib\\site-packages), Requirement.parse('azureml-core==1.0.72.*')).\nFailure while loading azureml_run_type_providers. Failed to load entrypoint azureml.ReusedStepRun = azureml.pipeline.core.run:StepRun._from_reused_dto with exception (azureml-core 1.3.0 (c:\\programdata\\anaconda3\\lib\\site-packages), Requirement.parse('azureml-core==1.0.72.*')).\nFailure while loading azureml_run_type_providers. Failed to load entrypoint azureml.StepRun = azureml.pipeline.core.run:StepRun._from_dto with exception (azureml-core 1.3.0 (c:\\programdata\\anaconda3\\lib\\site-packages), Requirement.parse('azureml-core==1.0.72.*')).\nFailure while loading azureml_run_type_providers. Failed to load entrypoint hyperdrive = azureml.train.hyperdrive:HyperDriveRun._from_run_dto with exception (azureml-telemetry 1.3.0 (c:\\programdata\\anaconda3\\lib\\site-packages), Requirement.parse('azureml-telemetry==1.0.72.*')).\n"
        }
      ],
      "source": [
        "import logging\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import azureml.core\n",
        "from azureml.core.experiment import Experiment\n",
        "from azureml.core.workspace import Workspace\n",
        "from azureml.core.dataset import Dataset\n",
        "from azureml.core.compute import AmlCompute\n",
        "from azureml.core.compute import ComputeTarget\n",
        "from azureml.core.run import Run\n",
        "from azureml.widgets import RunDetails\n",
        "from azureml.core.model import Model \n",
        "from helper import run_inference, get_result_df\n",
        "from azureml.train.automl import AutoMLConfig\n",
        "from sklearn.datasets import fetch_20newsgroups"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As part of the setup you have already created a <b>Workspace</b>. To run AutoML, you also need to create an <b>Experiment</b>. An Experiment corresponds to a prediction problem you are trying to solve, while a Run corresponds to a specific approach to the problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "                                                     \nSDK version      1.3.0                               \nSubscription ID  29b64be4-867b-40ee-a259-b58b97bfc26f\nWorkspace Name   data-science                        \nResource Group   data-science                        \nLocation         westeurope                          \nExperiment Name  automl-classification-text-dnn      ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>SDK version</th>\n      <td>1.3.0</td>\n    </tr>\n    <tr>\n      <th>Subscription ID</th>\n      <td>29b64be4-867b-40ee-a259-b58b97bfc26f</td>\n    </tr>\n    <tr>\n      <th>Workspace Name</th>\n      <td>data-science</td>\n    </tr>\n    <tr>\n      <th>Resource Group</th>\n      <td>data-science</td>\n    </tr>\n    <tr>\n      <th>Location</th>\n      <td>westeurope</td>\n    </tr>\n    <tr>\n      <th>Experiment Name</th>\n      <td>automl-classification-text-dnn</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "ws = Workspace.from_config()\n",
        "\n",
        "# Choose an experiment name.\n",
        "experiment_name = 'automl-classification-text-dnn'\n",
        "\n",
        "experiment = Experiment(ws, experiment_name)\n",
        "\n",
        "output = {}\n",
        "output['SDK version'] = azureml.core.VERSION\n",
        "output['Subscription ID'] = ws.subscription_id\n",
        "output['Workspace Name'] = ws.name\n",
        "output['Resource Group'] = ws.resource_group\n",
        "output['Location'] = ws.location\n",
        "output['Experiment Name'] = experiment.name\n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "outputDf = pd.DataFrame(data = output, index = [''])\n",
        "outputDf.T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set up a compute cluster\n",
        "This section uses a user-provided compute cluster (named \"dnntext-cluster\" in this example). If a cluster with this name does not exist in the user's workspace, the below code will create a new cluster. You can choose the parameters of the cluster as mentioned in the comments.\n",
        "\n",
        "Whether you provide/select a CPU or GPU cluster, AutoML will choose the appropriate DNN for that setup - BiLSTM or BERT text featurizer will be included in the candidate featurizers on CPU and GPU respectively.  If your goal is to obtain the most accurate model, we recommend you use GPU clusters since BERT featurizers usually outperform BiLSTM featurizers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Choose a name for your cluster.\n",
        "amlcompute_cluster_name = \"dnntext-cluster\"\n",
        "\n",
        "found = False\n",
        "# Check if this compute target already exists in the workspace.\n",
        "cts = ws.compute_targets\n",
        "if amlcompute_cluster_name in cts and cts[amlcompute_cluster_name].type == 'AmlCompute':\n",
        "    found = True\n",
        "    print('Found existing compute target.')\n",
        "    compute_target = cts[amlcompute_cluster_name]\n",
        "\n",
        "if not found:\n",
        "    print('Creating a new compute target...')\n",
        "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_NC6\", # CPU for BiLSTM, such as \"STANDARD_D2_V2\" \n",
        "                                                                # To use BERT (this is recommended for best performance), select a GPU such as \"STANDARD_NC6\" \n",
        "                                                                # or similar GPU option\n",
        "                                                                # available in your workspace\n",
        "                                                                max_nodes = 1)\n",
        "\n",
        "    # Create the cluster\n",
        "    compute_target = ComputeTarget.create(ws, amlcompute_cluster_name, provisioning_config)\n",
        "\n",
        "print('Checking cluster status...')\n",
        "# Can poll for a minimum number of nodes and for a specific timeout.\n",
        "# If no min_node_count is provided, it will use the scale settings for the cluster.\n",
        "compute_target.wait_for_completion(show_output = True, min_node_count = None, timeout_in_minutes = 20)\n",
        "\n",
        "# For a more detailed view of current AmlCompute status, use get_status()."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Get data\n",
        "For this notebook we will use 20 Newsgroups data from scikit-learn. We filter the data to contain four classes and take a sample as training data. Please note that for accuracy improvement, more data is needed. For this notebook we provide a small-data example so that you can use this template to use with your larger sized data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_dir = \"text-dnn-data\" # Local directory to store data\n",
        "blobstore_datadir = data_dir # Blob store directory to store data in\n",
        "target_column_name = 'y'\n",
        "feature_column_name = 'X'\n",
        "\n",
        "def get_20newsgroups_data():\n",
        "    '''Fetches 20 Newsgroups data from scikit-learn\n",
        "       Returns them in form of pandas dataframes\n",
        "    '''\n",
        "    remove = ('headers', 'footers', 'quotes')\n",
        "    categories = [\n",
        "        'alt.atheism',\n",
        "        'talk.religion.misc',\n",
        "        'comp.graphics',\n",
        "        'sci.space',\n",
        "        ]\n",
        "\n",
        "    data = fetch_20newsgroups(subset = 'train', categories = categories,\n",
        "                                    shuffle = True, random_state = 42,\n",
        "                                    remove = remove)\n",
        "    data = pd.DataFrame({feature_column_name: data.data, target_column_name: data.target})\n",
        "\n",
        "    data_train = data[:200]\n",
        "    data_test = data[200:300]    \n",
        "\n",
        "    data_train = remove_blanks_20news(data_train, feature_column_name, target_column_name)\n",
        "    data_test = remove_blanks_20news(data_test, feature_column_name, target_column_name)\n",
        "    \n",
        "    return data_train, data_test\n",
        "    \n",
        "def remove_blanks_20news(data, feature_column_name, target_column_name):\n",
        "    \n",
        "    data[feature_column_name] = data[feature_column_name].replace(r'\\n', ' ', regex=True).apply(lambda x: x.strip())\n",
        "    data = data[data[feature_column_name] != '']\n",
        "    \n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Fetch data and upload to datastore for use in training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Downloading 20news dataset. This may take a few minutes.\nINFO - Downloading 20news dataset. This may take a few minutes.\nDownloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\nINFO - Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\nUploading an estimated of 2 files\nUploading text-dnn-data\\test_data.csv\nUploading text-dnn-data\\train_data.csv\nUploaded text-dnn-data\\test_data.csv, 1 files out of an estimated total of 2\nUploaded text-dnn-data\\train_data.csv, 2 files out of an estimated total of 2\nUploaded 2 files\n"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "$AZUREML_DATAREFERENCE_4400f3cb9298496a9f79a31c6463b142"
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "data_train, data_test = get_20newsgroups_data()\n",
        "\n",
        "if not os.path.isdir(data_dir):\n",
        "    os.mkdir(data_dir)\n",
        "    \n",
        "train_data_fname = data_dir + '/train_data.csv'\n",
        "test_data_fname = data_dir + '/test_data.csv'\n",
        "\n",
        "data_train.to_csv(train_data_fname, index=False)\n",
        "data_test.to_csv(test_data_fname, index=False)\n",
        "\n",
        "datastore = ws.get_default_datastore()\n",
        "datastore.upload(src_dir=data_dir, target_path=blobstore_datadir,\n",
        "                    overwrite=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset = Dataset.Tabular.from_delimited_files(path = [(datastore, blobstore_datadir + '/train_data.csv')])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prepare AutoML run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This step requires an Enterprise workspace to gain access to this feature. To learn more about creating an Enterprise workspace or upgrading to an Enterprise workspace from the Azure portal, please visit our [Workspace page](https://docs.microsoft.com/azure/machine-learning/service/concept-workspace#upgrade)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "automl_settings = {\n",
        "    \"experiment_timeout_minutes\": 20,\n",
        "    \"primary_metric\": 'accuracy',\n",
        "    \"max_concurrent_iterations\": 4, \n",
        "    \"max_cores_per_iteration\": -1,\n",
        "    # \"enable_dnn\": True,\n",
        "    \"enable_early_stopping\": True,\n",
        "    \"validation_size\": 0.3,\n",
        "    \"verbosity\": logging.INFO,\n",
        "    \"enable_voting_ensemble\": False,\n",
        "    \"enable_stack_ensemble\": False,\n",
        "    \"preprocess\": True\n",
        "}\n",
        "\n",
        "automl_config = AutoMLConfig(task = 'classification',\n",
        "                             debug_log = 'automl_errors.log',\n",
        "                            #  compute_target=compute_target,\n",
        "                             training_data=train_dataset,\n",
        "                             label_column_name=target_column_name,\n",
        "                             **automl_settings\n",
        "                            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Submit AutoML Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Running on local machine\nParent Run ID: AutoML_03162ee0-86e1-40d4-b477-5ca50e7372cf\nCurrent status: DatasetFeaturization. Beginning to featurize the dataset.\nCurrent status: DatasetEvaluation. Gathering dataset statistics.\nCurrent status: FeaturesGeneration. Generating features for the dataset.\nCurrent status: DatasetFeaturizationCompleted. Completed featurizing the dataset.\n\n****************************************************************************************************\nDATA GUARDRAILS SUMMARY:\nFor more details, use API: run.get_guardrails()\n\nTYPE:         Class Balancing Detection\nSTATUS:       PASSED\nDESCRIPTION:  Classes are balanced in the training data.\n\nTYPE:         Missing Values Imputation\nSTATUS:       PASSED\nDESCRIPTION:  There were no missing values found in the training data.\n\nTYPE:         High Cardinality Feature Detection\nSTATUS:       DONE\nDESCRIPTION:  High cardinality inputs were detected in dataset and were featurized as text.\n\n****************************************************************************************************\nCurrent status: ModelSelection. Beginning model selection.\n\n****************************************************************************************************\nITERATION: The iteration being evaluated.\nPIPELINE: A summary description of the pipeline being evaluated.\nDURATION: Time taken for the current iteration.\nMETRIC: The result of computing score on the fitted pipeline.\nBEST: The best observed score thus far.\n****************************************************************************************************\n\n ITERATIONPIPELINEDURATION      METRIC      BEST\n"
        },
        {
          "output_type": "error",
          "ename": "ServiceException",
          "evalue": "ServiceException:\n\tMessage: Error occurred when trying to fetch next iteration from AutoML service.\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"code\": \"System\",\n        \"inner_error\": {\n            \"code\": \"Service\"\n        },\n        \"message\": \"Error occurred when trying to fetch next iteration from AutoML service.\"\n    }\n}",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mServiceException\u001b[0m                          Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-12-d1a7934b5858>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mautoml_run\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mautoml_config\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\azureml\\core\\experiment.py\u001b[0m in \u001b[0;36msubmit\u001b[1;34m(self, config, tags, **kwargs)\u001b[0m\n\u001b[0;32m    200\u001b[0m         \u001b[0msubmit_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_experiment_submit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_log_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"submit config {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m             \u001b[0mrun\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubmit_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mworkspace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtags\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[0mrun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_tags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\azureml\\train\\automl\\automlconfig.py\u001b[0m in \u001b[0;36m_automl_static_submit\u001b[1;34m(automl_config_object, workspace, experiment_name, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0min_process\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m             \u001b[0mautoml_run\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_default_execution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msettings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[0mrun_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRunConfiguration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\azureml\\train\\automl\\automlconfig.py\u001b[0m in \u001b[0;36m_default_execution\u001b[1;34m(experiment, automl_settings, fit_params, show_output)\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[0mautoml_estimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAzureAutoMLClient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msettings_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mautoml_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\azureml\\train\\automl\\_azureautomlclient.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, run_configuration, compute_target, X, y, sample_weight, X_valid, y_valid, sample_weight_valid, cv_splits_indices, show_output, existing_run, training_data, validation_data, _script_run, kwargs)\u001b[0m\n\u001b[0;32m    289\u001b[0m                                 \u001b[0mcv_splits_indices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv_splits_indices\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m                                 \u001b[0mexisting_run\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexisting_run\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight_valid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight_valid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 291\u001b[1;33m                                 training_data=training_data, validation_data=validation_data, _script_run=_script_run)\n\u001b[0m\u001b[0;32m    292\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_console_writer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprintln\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Running on remote compute: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_configuration\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\azureml\\train\\automl\\_azureautomlclient.py\u001b[0m in \u001b[0;36m_fit_local\u001b[1;34m(self, X, y, sample_weight, X_valid, y_valid, sample_weight_valid, cv_splits_indices, existing_run, training_data, validation_data, _script_run)\u001b[0m\n\u001b[0;32m    694\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Start iteration: {0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mlog_activity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivity_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTelemetryConstants\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFIT_ITERATION_NAME\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 696\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mci\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_configs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_configs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    697\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconstants\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCompleted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\azureml\\train\\automl\\_azureautomlclient.py\u001b[0m in \u001b[0;36m_fit_iteration\u001b[1;34m(self, ci, transformed_data_context, dataset, feature_configs)\u001b[0m\n\u001b[0;32m    739\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    740\u001b[0m         \u001b[1;31m#  Query Jasmine for the next set of pipelines to run\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 741\u001b[1;33m         \u001b[0mtask_dto\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_task\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    742\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    743\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtask_dto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_experiment_over\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\azureml\\train\\automl\\_azureautomlclient.py\u001b[0m in \u001b[0;36m_get_task\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1028\u001b[0m                 \u001b[0mlogging_utilities\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1029\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mServiceException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Error occurred when trying to fetch next iteration from AutoML service.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1030\u001b[1;33m                     \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1031\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1032\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_create_folders\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mServiceException\u001b[0m: ServiceException:\n\tMessage: Error occurred when trying to fetch next iteration from AutoML service.\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"code\": \"System\",\n        \"inner_error\": {\n            \"code\": \"Service\"\n        },\n        \"message\": \"Error occurred when trying to fetch next iteration from AutoML service.\"\n    }\n}"
          ]
        }
      ],
      "source": [
        "automl_run = experiment.submit(automl_config, show_output=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "automl_run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Displaying the run objects gives you links to the visual tools in the Azure Portal. Go try them!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Retrieve the Best Model\n",
        "Below we select the best model pipeline from our iterations, use it to test on test data on the same compute cluster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can test the model locally to get a feel of the input/output. This step may require additional package installations such as pytorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_run, fitted_model = automl_run.get_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can now see what text transformations are used to convert text data to features for this dataset, including deep learning transformations based on BiLSTM or Transformer (BERT is one implementation of a Transformer) models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_transformations_used = []\n",
        "for column_group in fitted_model.named_steps['datatransformer'].get_featurization_summary():\n",
        "    text_transformations_used.extend(column_group['Transformations'])\n",
        "text_transformations_used"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Deploying the model\n",
        "We now use the best fitted model from the AutoML Run to make predictions on the test set.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get results stats, extract the best model from AutoML run, download and register the resultant best model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "summary_df = get_result_df(automl_run)\n",
        "best_dnn_run_id = summary_df['run_id'].iloc[0]\n",
        "best_dnn_run = Run(experiment, best_dnn_run_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_dir = 'Model' # Local folder where the model will be stored temporarily\n",
        "if not os.path.isdir(model_dir):\n",
        "    os.mkdir(model_dir)\n",
        "    \n",
        "best_dnn_run.download_file('outputs/model.pkl', model_dir + '/model.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Register the model in your Azure Machine Learning Workspace. If you previously registered a model, please make sure to delete it so as to replace it with this new model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Register the model\n",
        "model_name = 'textDNN-20News'\n",
        "model = Model.register(model_path = model_dir + '/model.pkl',\n",
        "                       model_name = model_name,\n",
        "                       tags=None,\n",
        "                       workspace=ws)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate on Test Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now use the best fitted model from the AutoML Run to make predictions on the test set.  \n",
        "\n",
        "Test set schema should match that of the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_dataset = Dataset.Tabular.from_delimited_files(path = [(datastore, blobstore_datadir + '/test_data.csv')])\n",
        "\n",
        "# preview the first 3 rows of the dataset\n",
        "test_dataset.take(3).to_pandas_dataframe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_experiment = Experiment(ws, experiment_name + \"_test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "script_folder = os.path.join(os.getcwd(), 'inference')\n",
        "os.makedirs(script_folder, exist_ok=True)\n",
        "shutil.copy2('infer.py', script_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_run = run_inference(test_experiment, compute_target, script_folder, best_dnn_run, test_dataset,\n",
        "                 target_column_name, model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Display computed metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "RunDetails(test_run).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_run.wait_for_completion()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.Series(test_run.get_metrics())"
      ]
    }
  ],
  "metadata": {
    "authors": [
      {
        "name": "anshirga"
      }
    ],
    "compute": [
      "AML Compute"
    ],
    "datasets": [
      "None"
    ],
    "deployment": [
      "None"
    ],
    "exclude_from_index": false,
    "framework": [
      "None"
    ],
    "friendly_name": "DNN Text Featurization",
    "index_order": 2,
    "kernelspec": {
      "display_name": "Python 3.7.3 64-bit ('base': conda)",
      "language": "python",
      "name": "python37364bitbaseconda7dbc791cb36749bd83e95b68b2a99bb5"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3-final"
    },
    "tags": [
      "None"
    ],
    "task": "Text featurization using DNNs for classification"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}